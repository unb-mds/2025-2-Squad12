# ğŸ•·ï¸ Conceitos do Scrapy

## ğŸ“¦ Response
`response` Ã© o objeto gerado apÃ³s a raspagem da pÃ¡gina.

## ğŸ”— Response.follow
`response.follow` Ã© o mÃ©todo para seguir links e continuar raspando pÃ¡ginas. Ele aceita um link e o mÃ©todo de callback que serÃ¡ aplicado Ã  resposta dessa nova pÃ¡gina. Ã‰ assim que construÃ­mos a navegaÃ§Ã£o da nossa Spider.

## ğŸ¯ XPath ou CSS Selector
Os dois servem para buscar coisas especÃ­ficas no .xml ou .html por meio das tags.

**Exemplos:**
```python
response.xpath('//h1/text()').get()
response.css('h1::text').get()

## ğŸ“ Items
Ã‰ onde sÃ£o salvos os conteÃºdos retirados. VocÃª cria as variÃ¡veis para cada coisa que quer guardar. Quando for fazer o scrap, a spider vai criar uma instÃ¢ncia da classe de item que vocÃª salvou e vai usar ela pra salvar os dados.

## ğŸ”„ Pipeline
Ã‰ uma funcionalidade do Scrapy para processar dados antes de salvÃ¡-los, seja em um banco de dados ou um arquivo .csv.

**Funcionamento:**
- No arquivo `settings.py` vocÃª especifica quais pipelines estÃ£o ativos e qual serÃ¡ a ordem de execuÃ§Ã£o
- Cada pipeline Ã© uma classe que define mÃ©todos para processar os itens
- O pipeline pode ser configurado para salvar em vÃ¡rios formatos e destinos por meio da configuraÃ§Ã£o prÃ©via ou por linha de comando direto

## ğŸ¤– Robots.txt
`robots.txt` Ã© um arquivo que instrui robÃ´s (como as spiders) a ignorar certas Ã¡reas de uma pÃ¡gina. VocÃª pode configurar a spider para obedecÃª-lo habilitando o comando `ROBOTSTXT_OBEY` presente no `settings.py`.

**ObservaÃ§Ã£o:** Vou ignorar esse arquivo e os CAPTCHAs por enquanto.